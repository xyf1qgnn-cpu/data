{
  "id": "snapshot_1767779346953_yrw48y10r",
  "approvalId": "approval_1767779240126_0gt4b3fcu",
  "approvalTitle": "任务文档: PDF缓存处理优化",
  "version": 2,
  "timestamp": "2026-01-07T09:49:06.953Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# 任务列表: PDF缓存处理优化\n\n## 任务分解\n\n### Phase 1: 核心函数改造与新增\n\n- [ ] 1.1. 优化 call_vision_api 日志输出\n  - **文件**: `/home/thelya/Work/data/processing.py`\n  - **修改点**:\n    - 第963行：修改 payload 日志，显示图片数量而非base64内容\n    - 第975行：修改响应日志，超过1000字符时只显示前500字符+\"...\"\n  - **目的**: 提升日志可读性，减少日志文件大小\n  - **_Leverage**: 现有日志框架和logger实例\n  - **_Requirements**: 需求1\n  - **_Prompt**: |\n      Role: Python Developer with expertise in logging and debugging\n\n      Task: Optimize logging in call_vision_api function to improve readability and reduce log size.\n\n      Specific changes:\n      1. Around line 963: Replace payload logging with a summary showing image count, model name, and max_tokens\n      2. Around line 975: Truncate response content logging - if content exceeds 1000 characters, only log first 500 characters with ellipsis\n\n      Restrictions:\n      - Do not remove any existing error logging\n      - Maintain logging levels (debug for detailed info)\n      - Ensure all critical information is still logged\n      - No changes to function signature or core logic\n\n      Success criteria:\n      - Logs no longer contain base64 encoded strings\n      - Response previews work correctly for long responses\n      - All existing error handling and logging remains intact\n      - Log file size is significantly reduced for PDF processing\n\n- [ ] 1.2. 改造 process_pdf 函数支持双模式\n  - **文件**: `/home/thelya/Work/data/processing.py`\n  - **修改点**:\n    - 添加 mode 参数 (默认 \"full\" 保持向后兼容)\n    - 重构内部逻辑为两阶段：图片提取 → API调用\n    - 添加cache保存逻辑（保存到 ./cache/{pdf_name}/）\n    - 统一返回字典格式（不再是元组）\n    - 添加页数限制支持（max_pages 配置）\n  - **目的**: 支持两阶段处理，创建cache机制\n  - **_Leverage**: encode_images_to_base64, build_vision_payload, call_vision_api, convert_pdf_to_images, get_page_count\n  - **_Requirements**: 需求2, 需求7, 需求8\n  - **_Prompt**: |\n      Role: Python Developer with expertise in PDF processing and function refactoring\n\n      Task: Refactor process_pdf function to support dual-mode operation with cache functionality.\n\n      Key changes needed:\n      1. Add mode parameter with default \"full\" for backward compatibility\n      2. Extract image extraction logic into first phase\n      3. Save extracted images to ./cache/{pdf_name}/ directory as JPEG (quality=95)\n      4. Implement API call as second phase (when mode=\"full\")\n      5. Change return type to dictionary:\n         - mode=\"extract_only\": {\"cache_dir\": str, \"image_paths\": List[str]}\n         - mode=\"full\": {\"result\": Dict, \"cache_dir\": str}\n      6. Add max_pages support: check config for max_pages (default 25), limit pages if exceeded\n      7. Handle page selection: process pages 1 to (page_count-1) for multi-page PDFs, or 1 for single-page\n      8. Return None on failure (don't raise exceptions)\n\n      Restrictions:\n      - Maintain backward compatibility (default mode=\"full\")\n      - Do not duplicate existing logic - reuse encode_images_to_base64 and other helper functions\n      - Ensure proper error handling and logging\n      - Create cache directory if it doesn't exist (os.makedirs with exist_ok=True)\n      - Follow existing code style and patterns\n\n      Success criteria:\n      - mode=\"extract_only\" returns cache info without calling API\n      - mode=\"full\" performs complete processing like original function\n      - Images are correctly saved to cache directory\n      - Return format is unified as dictionary\n      - max_pages configuration is respected\n      - Existing functionality remains unchanged for mode=\"full\"\n\n- [ ] 1.3. 新增 process_from_cache 函数\n  - **文件**: `/home/thelya/Work/data/processing.py`\n  - **新增位置**: 在 process_pdf 函数之后\n  - **功能**: 从cache读取图片并调用API处理\n  - **目的**: 实现第二阶段处理，支持失败重试\n  - **_Leverage**: Image.open, encode_images_to_base64, build_vision_payload, call_vision_api\n  - **_Requirements**: 需求3, 需求6\n  - **_Prompt**: |\n      Role: Python Developer with expertise in image processing and API integration\n\n      Task: Implement process_from_cache function for second-phase processing.\n\n      Function signature:\n      def process_from_cache(\n          cache_dir: str,\n          pdf_name: str,\n          image_paths: List[str],\n          client,\n          config: Dict[str, Any],\n          system_prompt: str\n      ) -> Optional[Dict[str, Any]]:\n\n      Implementation requirements:\n      1. Log cache processing info: cache directory name and image count\n      2. Read images from provided image_paths using Image.open()\n      3. Reuse encode_images_to_base64 to encode images (JPEG format)\n      4. Reuse build_vision_payload to construct API request\n      5. Reuse call_vision_api to call the vision API\n      6. Return API result on success, None on failure\n      7. Handle errors gracefully with proper logging\n\n      Restrictions:\n      - Do not modify cache directory or image files (read-only operation)\n      - Reuse existing functions - do not duplicate encoding/payload logic\n      - Add proper error handling for file reading failures\n      - Follow existing error logging patterns\n      - Include cache_dir in logs for traceability\n\n      Success criteria:\n      - Function successfully reads images from cache\n      - API is called with correct payload\n      - Result is properly returned\n      - Errors are logged with clear context\n      - Cache directory remains unchanged after processing\n\n- [ ] 1.4. 新增 archive_cache 函数\n  - **文件**: `/home/thelya/Work/data/main.py` （或 processing.py，建议放main.py）\n  - **功能**: 将cache目录压缩并归档到指定位置\n  - **目的**: 自动归档提取的图片，便于备份和后续使用\n  - **_Leverage**: shutil.make_archive, shutil.rmtree, datetime\n  - **_Requirements**: 需求4\n  - **_Prompt**: |\n      Role: Python Developer with expertise in file operations and archiving\n\n      Task: Implement archive_cache function to zip and archive cache directories.\n\n      Function signature:\n      def archive_cache(\n          cache_dir: str,\n          pdf_name: str,\n          batch_number: int,\n          archive_base: str\n      ) -> Optional[str]:\n\n      Implementation requirements:\n      1. Generate timestamp: datetime.now().strftime(\"%Y-%m-%d\")\n      2. Create archive directory: f\"{archive_base}/Dataset ({batch_number}) {date_str}/\"\n      3. Create zip file: f\"{archive_dir}/{pdf_name}_images.zip\"\n      4. Use shutil.make_archive to create zip (format='zip')\n      5. On success: delete cache directory with shutil.rmtree\n      6. On failure: catch exception, log error, keep cache directory, return None\n      7. Return path to created zip file on success\n\n      Restrictions:\n      - Ensure archive_base directory exists (create with os.makedirs if needed)\n      - Handle exceptions for zip creation and directory deletion\n      - Validate that cache_dir is within expected location (security)\n      - Do not archive if zip already exists (skip or overwrite based on design)\n      - Follow existing logging patterns\n\n      Success criteria:\n      - Zip file is correctly created with all cache contents\n      - Cache directory is deleted after successful archiving\n      - Archive path follows format: Dataset ({batch}) YYYY-MM-DD/{name}_images.zip\n      - Errors are properly caught and logged\n      - Function returns correct path or None\n      - Source directory is validated for security\n\n### Phase 2: 主流程更新与错误处理\n\n- [ ] 2.1. 更新 main.py 主流程调用逻辑\n  - **文件**: `/home/thelya/Work/data/main.py`\n  - **修改点**: 在处理循环中调用三阶段流程\n  - **目的**: 实现完整的三阶段处理（提取→API→归档）\n  - **_Leverage**: process_pdf, process_from_cache, archive_cache, state.json管理\n  - **_Requirements**: 需求5, 需求7\n  - **_Prompt**: |\n      Role: Python Developer with expertise in workflow orchestration and error handling\n\n      Task: Update main.py PDF processing loop to implement three-stage processing workflow.\n\n      Changes needed in the PDF processing loop:\n      1. Stage 1: Call process_pdf with mode=\"extract_only\" to extract images\n      2. Check if cache_result is not None\n      3. Extract pdf_name from file_path using os.path.basename and os.path.splitext\n      4. Stage 2: Call process_from_cache with extracted cache_dir, pdf_name, and image_paths\n      5. Check if API result is not None\n      6. Stage 3: Call archive_cache with cache_dir, pdf_name, batch_number, and archive_destination from config\n      7. Update state.json after successful processing\n      8. Add comprehensive error handling for each stage\n      9. Move PDF to NotInput on permanent failures\n      10. Log each stage progress with clear messages\n\n      Restrictions:\n      - Maintain existing batch processing logic\n      - Do not break existing error handling patterns\n      - Ensure proper cleanup on failures\n      - Update state.json only on successful completion\n      - Follow existing logging format and levels\n      - Handle None returns from all three functions\n\n      Success criteria:\n      - Three-stage workflow is correctly implemented\n      - Errors at each stage are properly handled\n      - PDFs are moved to NotInput only on unrecoverable failures\n      - Cache is preserved when API fails (for retry)\n      - Progress is clearly logged at each stage\n      - State.json is updated correctly\n\n- [ ] 2.2. 添加命令行参数支持独立执行第二阶段\n  - **文件**: `/home/thelya/Work/data/main.py`\n  - **新增**: argparse 参数解析\n  - **目的**: 支持 --mode, --cache_dir, --pdf_name 参数\n  - **_Leverage**: argparse, sys.argv\n  - **_Requirements**: 需求6\n  - **_Prompt**: |\n      Role: Python Developer with expertise in CLI interfaces and command-line parsing\n\n      Task: Add command-line argument support for independent second-stage processing.\n\n      Add argparse configuration for:\n      1. --mode: choices=['full', 'extract_only', 'process_from_cache'], default='full'\n      2. --cache_dir: required when mode='process_from_cache'\n      3. --pdf_name: required when mode='process_from_cache'\n\n      Implementation requirements:\n      1. Set up argument parser at the beginning of main()\n      2. Parse arguments and store in variables\n      3. When mode='process_from_cache':\n         - Validate cache_dir exists\n         - Scan directory for .jpg files (sorted)\n         - Skip stage 1 (image extraction)\n         - Call process_from_cache directly\n         - Check if already archived before archiving\n      4. When mode='extract_only':\n         - Call process_pdf with mode='extract_only' for all PDFs\n         - Skip API processing\n         - Do not archive\n      5. Maintain default behavior (mode='full')\n\n      Restrictions:\n      - Add validation: cache_dir must exist and contain jpg files\n      - Add validation: pdf_name must be valid filename\n      - Do not break existing default behavior\n      - Provide clear help messages for each argument\n      - Follow existing code structure and imports\n\n      Success criteria:\n      - All three modes work correctly\n      - Validation errors show clear messages\n      - Default mode maintains backward compatibility\n      - process_from_cache mode reads cache correctly\n      - extract_only mode skips API calls\n      - All arguments have proper help text\n\n### Phase 3: 配置管理与清理\n\n- [ ] 3.1. 更新 config.json 配置\n  - **文件**: `/home/thelya/Work/data/config.json`\n  - **修改点**:\n    - 删除 page_filtering 配置块\n    - 添加 max_pages 配置\n    - 确认 archive_destination 路径\n  - **目的**: 清理过时配置，添加新配置项\n  - **_Leverage**: 现有config.json结构\n  - **_Requirements**: 需求11\n  - **_Prompt**: |\n      Role: Configuration Manager with expertise in JSON configuration files\n\n      Task: Update config.json to remove obsolete configurations and add new settings.\n\n      Changes required:\n      1. Remove entire \"page_filtering\" configuration block (lines 13-56)\n      2. Add \"max_pages\" to \"processing_settings\" (default: 25)\n      3. Verify \"archive_destination\" path in \"paths\" section exists and is correct:\n         - Current: \"/mnt/e/Documents/data_extracted\"\n      4. Verify \"processing_settings\" contains:\n         - short_paper_threshold\n         - max_scan_limit\n         - image_dpi\n         - enable_smart_filtering (can be removed or kept for backward compatibility)\n         - absolute_max_pages (can be removed or repurposed)\n\n      Restrictions:\n      - Maintain valid JSON syntax\n      - Do not remove other unrelated configurations\n      - Keep existing structure and formatting\n      - Ensure all paths are absolute and correct\n      - Backup config.json before modification\n\n      Success criteria:\n      - config.json is valid JSON (no syntax errors)\n      - page_filtering block is completely removed\n      - max_pages is added with correct default value\n      - All existing required configurations are preserved\n      - File passes JSON validation\n\n- [ ] 3.2. 删除过时函数和代码\n  - **文件**: `/home/thelya/Work/data/processing.py`\n  - **删除内容**:\n    - get_smart_pages_to_process 函数\n    - 所有页面筛选相关的代码\n  - **目的**: 清理不再使用的代码\n  - **_Leverage**: git history (for reference if needed)\n  - **_Requirements**: 需求10\n  - **_Prompt**: |\n      Role: Code Maintenance Engineer with expertise in code cleanup and refactoring\n\n      Task: Remove obsolete functions and code related to smart page filtering.\n\n      Code to remove:\n      1. get_smart_pages_to_process function (around lines 112-222)\n      2. Any remaining references to page_filtering logic in other functions\n      3. Any dead code detected after function removal\n\n      Verification steps:\n      1. Search for all references to \"page_filtering\" in processing.py\n      2. Search for all references to \"get_smart_pages_to_process\"\n      3. Remove function definition and all calls\n      4. Update or remove any related code\n      5. Ensure no breaking changes to remaining code\n\n      Restrictions:\n      - Do not remove functions that are still in use\n      - Verify no other files import the removed functions\n      - Check imports at the top of the file\n      - Run basic tests after removal\n      - Keep git commit history clean\n\n      Success criteria:\n      - get_smart_pages_to_process function is completely removed\n      - No references to page_filtering logic remain\n      - Code compiles without errors\n      - All tests pass (if any)\n      - No functional regression in remaining features\n\n### Phase 4: 测试与验证\n\n- [ ] 4.1. 编写单元测试\n  - **文件**: `/home/thelya/Work/data/test_pdf_cache.py` (新建)\n  - **测试内容**:\n    - process_pdf 两种模式的测试\n    - process_from_cache 正常和失败场景\n    - archive_cache 归档功能测试\n  - **目的**: 确保代码质量，便于后续维护\n  - **_Leverage**: pytest, unittest.mock\n  - **_Requirements**: (测试需求)\n  - **_Prompt**: |\n      Role: QA Engineer with expertise in Python testing and pytest framework\n\n      Task: Create comprehensive unit tests for new and modified functions.\n\n      Test coverage needed:\n      1. Test process_pdf with mode=\"extract_only\":\n         - Verify cache directory is created\n         - Verify correct number of images saved\n         - Verify return format {\"cache_dir\", \"image_paths\"}\n      2. Test process_pdf with mode=\"full\":\n         - Verify complete processing occurs\n         - Verify return format {\"result\", \"cache_dir\"}\n         - Mock API call to avoid actual API usage\n      3. Test process_pdf with max_pages limit:\n         - Create PDF with more pages than max_pages\n         - Verify only max_pages are processed\n         - Verify warning is logged\n      4. Test process_from_cache success:\n         - Create mock cache with sample images\n         - Verify images are read correctly\n         - Mock API call\n         - Verify result is returned\n      5. Test process_from_cache failure:\n         - Mock API call to raise exception\n         - Verify None is returned\n         - Verify error is logged\n      6. Test archive_cache success:\n         - Create temp cache directory with images\n         - Verify zip is created\n         - Verify cache is deleted after archiving\n         - Verify correct path is returned\n      7. Test archive_cache failure:\n         - Mock shutil.make_archive to raise exception\n         - Verify None is returned\n         - Verify cache is not deleted\n      8. Test archive_cache duplicate:\n         - Call archive_cache twice on same cache\n         - Verify second call behavior (skip or overwrite)\n\n      Restrictions:\n      - Use pytest framework\n      - Use unittest.mock for mocking API calls and file operations\n      - Create temp directories for testing (use tmpdir fixture)\n      - Clean up test artifacts after tests\n      - Do not make actual API calls in tests\n      - Test both success and failure scenarios\n\n      Success criteria:\n      - All test cases pass\n      - Code coverage >80% for new functions\n      - Tests are isolated and repeatable\n      - Mock external dependencies properly\n      - Clear test names and docstrings\n\n- [ ] 4.2. 集成测试与部署验证\n  - **测试内容**:\n    - 端到端完整流程测试\n    - 失败重试场景测试\n    - 性能对比测试\n  - **目的**: 验证系统整体功能和性能\n  - **_Leverage**: 真实PDF文件, 计时器, 日志分析\n  - **_Requirements**: (所有需求)\n  - **_Prompt**: |\n      Role: QA Engineer with expertise in integration testing and performance validation\n\n      Task: Perform comprehensive integration testing and deployment validation.\n\n      Test scenarios:\n      1. End-to-end happy path:\n         - Process 3-5 real PDF files\n         - Verify all three stages execute correctly\n         - Verify data is extracted correctly\n         - Verify cache is created and archived\n         - Verify state.json is updated\n      2. Failure and retry scenario:\n         - Process PDF with simulated API failure\n         - Verify cache is preserved\n         - Retry with process_from_cache mode\n         - Verify successful second attempt\n      3. Performance comparison:\n         - Measure time for full processing (including image extraction)\n         - Measure time for second stage only (from cache)\n         - Verify second stage is significantly faster\n      4. Log analysis:\n         - Process multiple PDFs\n         - Analyze log file size\n         - Verify base64 content is removed\n         - Verify response truncation works\n      5. Batch processing:\n         - Process 10+ PDFs in batch\n         - Verify batch number increments correctly\n         - Verify all caches are archived\n         - Check for memory leaks\n      6. Edge cases:\n         - Single page PDF\n         - Very large PDF (25+ pages)\n         - PDF with no extractable data\n         - Corrupted PDF file\n\n      Restrictions:\n      - Use test PDFs that are representative of real data\n      - Do not use production API keys in tests\n      - Monitor disk space during testing\n      - Clean up test artifacts after completion\n      - Document all test results\n\n      Success criteria:\n      - All end-to-end tests pass\n      - Second stage processing is at least 50% faster than full processing\n      - Log files are significantly smaller (at least 90% reduction in size)\n      - Batch processing completes without errors\n      - All edge cases are handled gracefully\n      - No memory leaks detected\n\n### Phase 5: 文档与部署\n\n- [ ] 5.1. 更新文档和 README\n  - **文件**: `/home/thelya/Work/data/README.md`\n  - **更新内容**:\n    - 新增缓存机制说明\n    - 两阶段处理流程说明\n    - 命令行参数使用说明\n    - 归档结构说明\n  - **目的**: 帮助用户理解新功能\n  - **_Leverage**: 现有README结构\n  - **_Prompt**: |\n      Role: Technical Writer with expertise in software documentation\n\n      Task: Update README.md to document new cache features and two-phase processing.\n\n      Documentation sections to add/update:\n      1. Overview of cache mechanism:\n         - Explain two-phase processing concept\n         - Benefits (reliability, performance)\n         - Cache directory structure\n      2. Configuration section:\n         - Document max_pages setting\n         - Document archive_destination path\n      3. Usage section:\n         - Default mode (full processing)\n         - Extract only mode with example\n         - Process from cache mode with example\n         - Command-line arguments table\n      4. Workflow description:\n         - Step-by-step processing flow\n         - Error handling and retry process\n         - Archive structure and location\n      5. Troubleshooting:\n         - Cache directory location\n         - Manual retry process\n         - Log file location and analysis\n\n      Restrictions:\n      - Maintain existing README structure\n      - Use clear, concise language\n      - Include code examples for CLI usage\n      - Update table of contents\n      - Keep documentation in sync with code\n\n      Success criteria:\n      - All new features are documented\n      - Examples are accurate and tested\n      - Documentation is clear and comprehensive\n      - Formatting is consistent\n      - Users can understand and use new features from docs\n\n---\n\n## 实施优先级\n\n### Phase 1 (高优先级) - 核心功能\n- 任务 1.1: 优化日志\n- 任务 1.2: 改造 process_pdf\n- 任务 1.3: 新增 process_from_cache\n- 任务 1.4: 新增 archive_cache\n\n### Phase 2 (高优先级) - 集成\n- 任务 2.1: 更新主流程\n- 任务 2.2: 命令行参数\n\n### Phase 3 (中优先级) - 清理\n- 任务 3.1: 更新配置\n- 任务 3.2: 删除过时代码\n\n### Phase 4 (中优先级) - 测试\n- 任务 4.1: 单元测试\n- 任务 4.2: 集成测试\n\n### Phase 5 (低优先级) - 文档\n- 任务 5.1: 更新文档\n\n---\n\n## 时间估算\n\n| 阶段 | 任务 | 时间估算 |\n|------|------|----------|\n| Phase 1 | 任务 1.1-1.4 | 3-4小时 |\n| Phase 2 | 任务 2.1-2.2 | 2-3小时 |\n| Phase 3 | 任务 3.1-3.2 | 1小时 |\n| Phase 4 | 任务 4.1-4.2 | 3-4小时 |\n| Phase 5 | 任务 5.1 | 30分钟 |\n| **总计** | | **10-13小时** (~1.5-2天) |\n\n---\n\n## 依赖关系\n\n```\n任务 1.1 (日志优化) → 无依赖\n任务 1.2 (process_pdf改造) → 无依赖\n任务 1.3 (process_from_cache) → 依赖 1.2\n任务 1.4 (archive_cache) → 无依赖\n\n任务 2.1 (主流程更新) → 依赖 1.2, 1.3, 1.4\n任务 2.2 (CLI参数) → 依赖 1.3\n\n任务 3.1 (配置更新) → 无依赖\n任务 3.2 (代码清理) → 依赖 1.2, 2.1 (确保不再使用旧函数)\n\n任务 4.1 (单元测试) → 依赖所有实现任务\n任务 4.2 (集成测试) → 依赖 4.1\n\n任务 5.1 (文档) → 依赖所有功能实现\n```\n\n---\n\n## 验收标准\n\n所有任务完成后需要验证：\n\n1. ✅ 日志中不包含base64编码内容\n2. ✅ process_pdf 支持 mode 参数且向后兼容\n3. ✅ 图片正确保存到 ./cache/{pdf_name}/\n4. ✅ process_from_cache 可以独立调用\n5. ✅ archive_cache 正确创建zip并清理cache\n6. ✅ 主流程执行三阶段处理\n7. ✅ 命令行参数支持三种模式\n8. ✅ 配置文件中已删除 page_filtering\n9. ✅ 所有单元测试通过\n10. ✅ 集成测试验证性能提升\n11. ✅ 文档已更新\n",
  "fileStats": {
    "size": 23648,
    "lines": 590,
    "lastModified": "2026-01-07T09:47:16.191Z"
  },
  "comments": []
}