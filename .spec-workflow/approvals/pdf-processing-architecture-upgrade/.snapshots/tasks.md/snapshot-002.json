{
  "id": "snapshot_1767708047100_agvd3q14x",
  "approvalId": "approval_1767707907596_6v1n82wyk",
  "approvalTitle": "PDF Processing Architecture Upgrade Tasks",
  "version": 2,
  "timestamp": "2026-01-06T14:00:47.100Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Tasks Document\n\n## Phase 1: Configuration Infrastructure\n\n- [ ] 1.1 Refactor config.json structure\n  - File: config.json\n  - Implement new nested structure with api_settings, processing_settings, and paths\n  - Update all existing config references in main.py\n  - Add manual_review_path configuration\n  - _Leverage: existing config.json loading pattern in main.py\n  - _Requirements: 1\n  - _Prompt: Role: Python Developer with expertise in configuration management and JSON schema design | Task: Refactor config.json to implement nested structure with api_settings, processing_settings, and paths sections following requirement 1, updating all references in main.py to use new structure | Restrictions: Must maintain backward compatibility for existing path configs, use clear and intuitive naming, validate required fields on load | Success: New config structure is implemented and validated, all existing config references are updated, application can load both old and new structures gracefully\n\n- [ ] 1.2 Create configuration validation module\n  - File: config_manager.py (new)\n  - Implement config validation with clear error messages for missing required fields\n  - Add API settings validation (api_key, base_url, model_name)\n  - Add processing settings validation with reasonable defaults\n  - _Leverage: existing load_config() pattern from main.py\n  - _Requirements: 1, 6\n  - _Prompt: Role: Python Developer specializing in configuration validation and error handling | Task: Create configuration validation module following requirements 1 and 6, implementing comprehensive validation for API settings and processing parameters with clear error messages | Restrictions: Must validate all required fields, provide helpful error messages in Chinese for Chinese users, use type hints for better code quality | Success: All config validations work correctly, clear error messages guide users to fix issues, validation covers all required and optional fields\n\n## Phase 2: Vision Processing Infrastructure\n\n- [ ] 2.1 Install and configure pdf2image and dependencies\n  - Install pdf2image and pillow packages\n  - Add poppler dependency check on application startup\n  - Provide Chinese installation instructions for different platforms\n  - _Leverage: shutil.which() for dependency checking\n  - _Requirements: 6\n  - _Prompt: Role: DevOps Engineer with expertise in Python package management and system dependencies | Task: Install pdf2image and configure poppler dependency checking following requirement 6, with platform-specific installation instructions | Restrictions: Must not hardcode platform assumptions, provide clear actionable instructions, handle both apt and manual installation | Success: pdf2image is installed and working, poppler check provides clear instructions, application starts successfully with dependencies met\n\n- [ ] 2.2 Create PDF image conversion utility\n  - File: processing.py (modify existing)\n  - Implement convert_pdf_to_images(pdf_path, page_limit) function\n  - Add get_page_count(pdf_path) function using pdf2image\n  - Handle image format conversion and DPI settings\n  - _Leverage: pdf2image.convert_from_path(), PIL.Image\n  - _Requirements: 2, 3\n  - _Prompt: Role: Python Developer with expertise in PDF processing and image manipulation | Task: Create PDF image conversion utility following requirements 2 and 3, implementing functions to convert PDF pages to images and count pages accurately | Restrictions: Must handle page limits correctly, support configurable DPI, manage memory efficiently for large PDFs, provide clear error messages for invalid PDFs | Success: PDFs are converted to images correctly, page counting is accurate, memory usage is reasonable, errors are handled gracefully\n\n- [ ] 2.3 Implement base64 image encoding\n  - File: processing.py (continue from task 2.2)\n  - Create encode_image_to_base64(image) function\n  - Handle image format conversion for optimal transmission\n  - _Leverage: base64 module, io.BytesIO\n  - _Requirements: 2\n  - _Prompt: Role: Python Developer with expertise in data encoding and optimization | Task: Implement base64 image encoding following requirement 2, creating efficient functions to convert PIL Images to base64 strings for API transmission | Restrictions: Must optimize image size for transmission, maintain image quality, handle multiple image formats, ensure efficient memory usage | Success: Images are encoded correctly to base64, file sizes are optimized, encoding is efficient and reliable\n\n## Phase 3: AI Integration\n\n- [ ] 3.1 Update OpenAI client initialization\n  - File: main.py\n  - Replace instructor-patched DeepSeek client with standard OpenAI client\n  - Load credentials from config.json api_settings\n  - Remove hardcoded API_KEY, BASE_URL, MODEL_NAME constants\n  - _Leverage: openai.OpenAI client, config loading functions\n  - _Requirements: 1, 2\n  - _Prompt: Role: Python Developer with expertise in API integrations and OpenAI-compatible interfaces | Task: Update OpenAI client initialization following requirements 1 and 2, replacing hardcoded values with config.json loaded settings | Restrictions: Must remove all hardcoded constants, maintain error handling patterns, support both old and new config structures temporarily | Success: OpenAI client is initialized with config values, no hardcoded credentials remain, error handling works correctly\n\n- [ ] 3.2 Create vision API payload builder\n  - File: processing.py\n  - Implement build_vision_payload(images, system_prompt) function\n  - Construct OpenAI-compatible vision format with image_url elements\n  - Add text instruction for multi-page analysis\n  - _Leverage: SYSTEM_PROMPT constant from main.py\n  - _Requirements: 2, 5\n  - _Prompt: Role: Python Developer specializing in API payload construction and prompt engineering | Task: Create vision API payload builder following requirements 2 and 5, constructing OpenAI-compatible vision format with system prompt and multiple images | Restrictions: Must follow OpenAI vision API specification, maintain system prompt exactly as provided, optimize token usage, handle multiple images efficiently | Success: Payload is correctly formatted for vision API, system prompt is preserved, all images are included, token usage is optimized\n\n- [ ] 3.3 Implement vision API call function\n  - File: processing.py\n  - Create call_vision_api(payload, client) function\n  - Handle API errors with exponential backoff retry\n  - Parse JSON response and handle parsing errors\n  - _Leverage: openai.OpenAI client, json parsing\n  - _Requirements: 2, 4\n  - _Prompt: Role: Python Developer with expertise in API integration and error handling | Task: Implement vision API call function following requirements 2 and 4, with error handling and retry logic | Restrictions: Must implement exponential backoff, handle network errors gracefully, parse JSON responses correctly, maintain batch processing on failures | Success: API calls work reliably, errors are handled with retries, JSON parsing is robust, failures don't stop batch processing\n\n## Phase 4: Adaptive Processing Strategy\n\n- [ ] 4.1 Implement adaptive page scanning logic\n  - File: processing.py\n  - Create should_scan_all_pages(page_count, threshold) function\n  - Create get_pages_to_process(page_count, config) function\n  - Implement decision logic for short vs long papers\n  - _Leverage: processing_settings from config.json\n  - _Requirements: 3\n  - _Prompt: Role: Python Developer with expertise in algorithm design and configuration-driven logic | Task: Implement adaptive page scanning logic following requirement 3, creating functions to determine scanning strategy based on page count and thresholds | Restrictions: Must use config values, handle edge cases (page count unknown), log decisions clearly, maintain batch processing efficiency | Success: Logic correctly determines scanning strategy, decisions are logged, edge cases handled, efficiency is maintained\n\n- [ ] 4.2 Create main process_pdf() function\n  - File: processing.py\n  - Implement main entry point orchestrating entire vision processing flow\n  - Integrate page counting, image conversion, encoding, API calls\n  - Return parsed JSON in same format as existing extract_data_with_ai()\n  - _Leverage: All functions from tasks 2.2, 2.3, 3.2, 3.3, 4.1\n  - _Requirements: 2, 3, 4, 5\n  - _Prompt: Role: Senior Python Developer with expertise in orchestration and integration | Task: Create main process_pdf() function following requirements 2, 3, 4, and 5, orchestrating entire vision processing pipeline | Restrictions: Must maintain same return format as existing function, handle all error scenarios, log processing steps clearly, integrate all components seamlessly | Success: process_pdf() works end-to-end, maintains compatibility with existing code, handles errors gracefully, logs are clear and informative\n\n## Phase 5: Error Handling and Edge Cases\n\n- [ ] 5.1 Implement zero-data detection and handling\n  - File: main.py\n  - Add check for empty Group_A, Group_B, and Group_C arrays\n  - Move files to manual_review_path with yellow warning message\n  - Create manual review directory if it doesn't exist\n  - _Leverage: move_failed_file pattern\n  - _Requirements: 4\n  - _Prompt: Role: Python Developer with expertise in edge case handling and user experience | Task: Implement zero-data detection and handling following requirement 4, creating logic to detect empty extraction results and move files appropriately | Restrictions: Must check all groups are empty, use colored warning messages, move to new manual_review_path, continue batch processing | Success: Zero-data cases are detected correctly, files moved to appropriate location, warnings are clear and colored, batch processing continues\n\n- [ ] 5.2 Add comprehensive error handling\n  - File: main.py and processing.py\n  - Wrap vision processing calls in try/except blocks\n  - Add specific error messages for different failure types\n  - Ensure failed files continue to NotInput/ in appropriate cases\n  - _Leverage: existing move_failed_file and move_excluded_file functions\n  - _Requirements: 2, 4, 6\n  - _Prompt: Role: Python Developer with expertise in defensive programming and error handling | Task: Add comprehensive error handling throughout vision processing pipeline following requirements 2, 4, and 6 | Restrictions: Must not let errors stop batch processing, provide specific error messages, differentiate between different failure types, maintain existing error handling patterns | Success: All errors are handled gracefully, batch processing continues, error messages are clear and actionable, files are moved to appropriate directories\n\n## Phase 6: Integration and Cleanup\n\n- [ ] 6.1 Update main.py to use vision processing\n  - File: main.py\n  - Replace extract_text_from_pdf() and extract_data_with_ai() calls with process_pdf()\n  - Update workflow orchestration to handle new processing pipeline\n  - Update imports and remove unused imports (pdfplumber, processing.optimize_text_for_extraction)\n  - _Leverage: process_pdf() from processing.py\n  - _Requirements: 1, 2, 3, 4, 5\n  - _Prompt: Role: Python Developer with expertise in refactoring and integration | Task: Update main.py to use new vision processing pipeline following requirements 1, 2, 3, 4, and 5, replacing text-based extraction with vision-based processing | Restrictions: Must maintain existing workflow structure, remove unused imports and code, preserve error handling patterns, ensure backward compatibility where possible | Success: main.py uses new vision processing, old text extraction code is removed, workflow functions correctly, all features work as expected\n\n- [ ] 6.2 Clean up unused code and dependencies\n  - Files: main.py, processing.py\n  - Remove extract_text_from_pdf() function\n  - Remove extract_data_with_ai() function\n  - Remove unused imports (pdfplumber from main.py)\n  - Update requirements.txt to remove pdfplumber, add pdf2image, pillow\n  - _Leverage: git for version control, careful code review\n  - _Requirements: 1, 2\n  - _Prompt: Role: Python Developer with expertise in code cleanup and dependency management | Task: Clean up unused code and dependencies following requirements 1 and 2, removing text-based extraction functions and updating dependencies | Restrictions: Must not break existing functionality, keep code history in git, update documentation, ensure all imports are cleaned up | Success: Unused code is removed, dependencies are updated, code is cleaner and more maintainable, documentation is updated\n\n- [ ] 6.3 Update documentation and add comments\n  - File: main.py, processing.py, config.json\n  - Update docstrings to reflect vision-based processing\n  - Add inline comments explaining new logic and parameters\n  - Update README.md with new architecture description\n  - Add configuration examples in config.json comments\n  - _Leverage: existing docstring patterns\n  - _Requirements: 1, 2, 3, 4\n  - _Prompt: Role: Technical Writer with expertise in code documentation and user guides | Task: Update documentation and add comments following requirements 1, 2, 3, and 4, explaining the new vision-based architecture | Restrictions: Must maintain existing documentation style, explain complex logic clearly, update all relevant documentation, use consistent terminology | Success: Documentation is clear and comprehensive, comments explain complex logic, users can understand the new architecture, configuration examples are helpful\n\n## Phase 7: Testing and Validation\n\n- [ ] 7.1 Test short paper processing (≤15 pages)\n  - Use sample short CFST paper\n  - Verify all pages are converted and processed\n  - Check that extraction works correctly\n  - Verify results in Excel output\n  - _Requirements: 3\n  - _Prompt: Role: QA Engineer with expertise in functional testing and validation | Task: Test short paper processing following requirement 3, verifying that papers with ≤15 pages are fully scanned and processed correctly | Restrictions: Must use real CFST papers, verify page counting accuracy, confirm all pages are processed, validate extraction quality | Success: Short papers are processed correctly, all pages are scanned, extraction works as expected, results are accurate\n\n- [ ] 7.2 Test long paper processing (>15 pages)\n  - Use sample long CFST paper (>15 pages)\n  - Verify only first 10 pages are processed (configurable)\n  - Check that warning is logged about truncation\n  - Validate that extraction still works with partial scanning\n  - _Requirements: 3\n  - _Prompt: Role: QA Engineer with expertise in boundary testing and performance validation | Task: Test long paper processing following requirement 3, verifying that papers with >15 pages are truncated correctly and warnings are logged | Restrictions: Must test with actual long papers, verify truncation logic, confirm warnings appear, ensure extraction quality is maintained | Success: Long papers are truncated correctly, warnings are logged, processing is efficient, extraction quality is acceptable\n\n- [ ] 7.3 Test zero-data detection\n  - Use non-CFST paper or paper without extractable data\n  - Verify yellow warning is displayed\n  - Check that file is moved to Manual_Review/ directory\n  - Confirm batch processing continues\n  - _Requirements: 4\n  - _Prompt: Role: QA Engineer with expertise in edge case testing and error handling | Task: Test zero-data detection following requirement 4, verifying that papers without extractable data are handled correctly | Restrictions: Must test with various non-CFST papers, verify warning messages, confirm file movement, ensure no processing interruption | Success: Zero-data cases are detected correctly, warnings are clear, files moved to correct location, batch processing continues\n\n- [ ] 7.4 Test dependency checks\n  - Test without poppler installed\n  - Verify clear error message in Chinese\n  - Test with invalid API configuration\n  - Verify appropriate error messages\n  - _Requirements: 6\n  - _Prompt: Role: QA Engineer with expertise in system testing and error validation | Task: Test dependency checks following requirement 6, verifying that missing dependencies and invalid configurations produce clear error messages | Restrictions: Must test on clean environment, verify error messages are actionable, test various configuration errors, ensure proper application shutdown | Success: Error messages are clear and helpful, users can understand and fix issues, application exits gracefully on critical errors\n\n- [ ] 7.5 Validate JSON output format\n  - Test that AI returns correct JSON structure\n  - Verify is_valid and reason fields are present\n  - Check that all specimen groups follow correct schema\n  - Validate numeric fields have units removed\n  - _Requirements: 5\n  - _Prompt: Role: QA Engineer with expertise in data validation and schema verification | Task: Validate JSON output format following requirement 5, verifying that AI responses match expected schema and all fields are correctly formatted | Restrictions: Must verify all JSON fields, check data types, validate numeric field formatting, ensure schema compliance | Success: JSON output matches expected format, all fields are present and correctly typed, numeric fields are formatted properly, validation passes\n",
  "fileStats": {
    "size": 17226,
    "lines": 191,
    "lastModified": "2026-01-06T13:58:22.795Z"
  },
  "comments": []
}