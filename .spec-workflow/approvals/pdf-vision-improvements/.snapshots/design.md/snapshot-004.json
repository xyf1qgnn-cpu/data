{
  "id": "snapshot_1767716257199_7i8o70msj",
  "approvalId": "approval_1767716231403_uufcci4ok",
  "approvalTitle": "AI视觉处理改进设计（修订版）- JSON约束和智能页面筛选",
  "version": 4,
  "timestamp": "2026-01-06T16:17:37.199Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Design Document\n\n## Overview\n\n本设计针对CFST数据提取系统的两大核心问题提供技术解决方案：\n\n1. **Token溢出问题**：通过强化Prompt约束，强制AI模型输出极简JSON格式（reason字段为双引号或最多10个词）\n2. **页面浪费问题**：实现智能页面筛选机制，仅处理最可能包含数据的页面\n\n解决方案采用两阶段架构设计，在保留现有视觉处理流程的基础上，新增轻量级的文本侦察阶段，实现成本与精度的平衡。\n\n## Steering Document Alignment\n\n### Technical Standards\n本设计遵循以下技术标准：\n- **模块化设计**：智能筛选功能独立封装，与视觉处理模块解耦\n- **配置驱动**：所有阈值、权重、关键词列表均通过config.json配置\n- **向后兼容**：不修改现有处理流程，新增能力作为可选优化\n- **错误隔离**：阶段一失败自动回退到阶段二，确保系统可用性\n\n### Project Structure\n设计遵循现有项目结构：\n- `processing.py`：新增智能筛选函数（保持原有视觉处理逻辑不变）\n- `config.json`：扩展配置项，包含评分权重、关键词列表等\n- `main.py`：更新System Prompt（不改变核心工作流程）\n\n## Code Reuse Analysis\n\n### Existing Components to Leverage\n- **`process_pdf()`函数**：现有视觉处理的主入口，扩展为支持选择性页面处理\n- **`config.json`结构**：已有的嵌套配置结构，新增`page_filtering`配置段\n- **`SYSTEM_PROMPT`**：现有系统提示词，增加JSON输出约束说明\n- **OpenAI客户端**：已初始化的API客户端，接口无需修改\n\n### Integration Points\n- **`processing.py`入口**：新增`get_smart_pages_to_process()`函数，返回筛选后的页面列表\n- **`config_manager.py`**：新增配置验证规则，确保评分权重合法\n- **主处理流程**：在`process_pdf()`内增加阶段一（文本侦察），必要时跳过阶段二\n\n## Architecture\n\n### 两阶段处理流程图\n\n```mermaid\ngraph TD\n    A[PDF文件] --> B{页数检查}\n    B -->|<10页| C[跳过筛选]\n    B -->|>10页| D[阶段一：文本侦察]\n    C --> H[转换所有页为图片]\n    D --> E[按关键词打分]\n    E --> F[选择Top N页]\n    F --> G[阶段二：视觉提取]\n    H --> G\n    G --> I[AI处理]\n    I --> J[返回JSON结果]\n\n    style D fill:#e3f2fd\n    style G fill:#e8f5e9\n    style F fill:#fff3e0\n```\n\n### 智能页面筛选流程\n\n```mermaid\ngraph TD\n    A[输入PDF] --> B[提取所有页文本]\n    B --> C[初始化权重]\n    C --> D{遍历每页}\n    D --> E[检测表格标题]\n    E -->|匹配| F[+10分]\n    D --> G[检测数据词汇]\n    G -->|匹配| H[+5分]\n    D --> I[检测参考文献]\n    I -->|匹配| J[-5分]\n    D --> K[无匹配]\n    K --> L[+1分]\n    F --> M[添加到得分表]\n    H --> M\n    J --> M\n    L --> M\n    M --> D\n    D --> N{完成所有页?}\n    N -->|否| D\n    N -->|是| O[排序页面得分]\n    O --> P[选择Top N页]\n    P --> Q[强制包含第1页]\n    Q --> R[返回页面列表 + 分数]\n\n    style A fill:#f3e5f5\n    style R fill:#e1f5e1\n```\n\n### Modular Design Principles\n\n**单文件职责**：\n- `processing.py`：只负责PDF处理和页面筛选（规模约600行）\n- `config_manager.py`：只负责配置验证（规模约200行）\n- `main.py`：只负责流程编排和日志（保持现有规模）\n\n**组件隔离**：\n- 智能筛选功能独立成`get_smart_pages_to_process()`函数，可单独测试\n- 评分算法独立成`score_page_content()`函数，便于调整权重\n- 配置解析隔离，确保新增配置不会影响旧配置\n\n**服务层分离**：\n- 数据访问层：pdfplumber提取文本（阶段一）、pdf2image转换图片（阶段二）\n- 业务逻辑层：页面评分算法、页面选择逻辑\n- API调用层：OpenAI/Gemini视觉处理（与现有流程一致）\n\n## Components and Interfaces\n\n### Component 1: PDF Text Extraction (阶段一基础)\n\n**Purpose**：使用pdfplumber快速提取PDF每页文本内容，用于评分\n\n**Interfaces**：\n```python\ndef extract_page_texts(pdf_path: str) -> Dict[int, str]:\n    \"\"\"\n    提取PDF所有页面的文本内容\n\n    参数：\n        pdf_path: PDF文件路径\n\n    返回：\n        字典 {页码（从1开始）: 文本内容}\n\n    异常：\n        pdfplumber失败 -> 返回空字典，触发回退机制\n    \"\"\"\n```\n\n**Dependencies**：\n- `pdfplumber`: 文本提取库\n- `os`: 文件路径操作\n\n**Reuses**：\n- 现有的pdfplumber依赖（已在requirements.txt中）\n\n### Component 2: Page Scoring Algorithm (核心评分逻辑)\n\n**Purpose**：实现页面内容评分算法，识别高价值页面\n\n**Interfaces**：\n```python\ndef score_page_content(text: str, config: dict) -> int:\n    \"\"\"\n    计算单页内容的评分\n\n    参数：\n        text: 页面文本内容\n        config: 包含关键词权重的配置\n\n    返回：\n        整数得分（可能为负数）\n\n    评分逻辑：\n        - 表格标题匹配：+10分（可配置）\n        - 数据词汇匹配：+5分（可配置）\n        - 参考文献页：-5分（可配置）\n        - 普通文本：+1分\n    \"\"\"\n\ndef get_smart_pages_to_process(pdf_path: str, config: dict) -> Tuple[List[int], str, List[Dict]]:\n    \"\"\"\n    主函数：返回需要处理的页面列表\n\n    参数：\n        pdf_path: PDF文件路径\n        config: 配置对象\n\n    返回：\n        Tuple包含：\n        - 页面编号列表（从1开始）\n        - 策略描述字符串（用于日志）\n        - 调试信息列表（包含页面分数和选择原因）\n    \"\"\"\n```\n\n**Dependencies**：\n- `re`: 正则表达式匹配\n- `typing`: 类型注解\n\n**Reuses**：\n- config.json中定义的关键词列表\n- 现有的日志系统记录页面选择逻辑\n\n### Component 3: Prompt Optimization (JSON约束)\n\n**Purpose**：优化SYSTEM_PROMPT，强制AI输出符合约束的JSON格式\n\n**Interfaces**：\n```python\n# 在main.py中修改SYSTEM_PROMPT常量（完整版本）\nSYSTEM_PROMPT = \"\"\"\n# Role\n\n你是一个精通钢管混凝土（CFST）试验数据的结构工程专家助手。\n你正在使用先进的计算机视觉能力分析学术论文的页面图像（Images of Paper Pages）。\n\n# 0. Relevance & Validity Check (CRITICAL)\n\n**CRITICAL STEP**: 在提取详细数据前，请综合文字和图像信息判断文档是否符合要求。\n**符合要求的标准**（必须全部满足）：\n\\n1. **对象**: 必须是钢管混凝土 (CFST) 构件。\n2. **内容**: 必须包含 **试验数据 (Experimental Data/Test Results)** 。\n   - *Visual Hint*: 寻找包含 \"Test\", \"Experimental\" 标题的表格，或展示试件破坏模态（压溃、鼓曲）的**试验照片**。\\n3. **构件类型**: 必须是柱 (Columns/Stub columns)。\n\n**拒绝情况 (Rejection)**：\n\n- 纯有限元模拟 (FEA only) 且无试验验证 -> **拒绝**。\\n- 纯理论推导 (Analytical only) -> **拒绝**。\\n- 梁 (Beams) 或 节点 (Joints) -> **拒绝**。\\n\n**如果不符合要求**：\n输出一个空的 JSON 结构，status 标记为 false：\n`{ \"Group_A\": [], \"Group_B\": [], \"Group_C\": [], \"is_valid\": false, \"reason\": \"Not experimental CFST column paper\" }`\n\n# Task\n分析输入的论文页面图像，提取CFST构件的试验数据。\n**Core Strategy (Visual Processing)**:\n\\n1. 扫描所有图片，定位包含几何尺寸和试验结果的表格 (Tables)。\\n2. 以 **Specimen Label (试件编号)** 为唯一索引（Primary Key）。\\n3. 如果数据分散在不同表格（例如尺寸在 Table 1，承载力在 Table 2），请根据 Specimen Label 将它们合并。\\n\n# 1. Classification & Geometry Mapping Rules (Strict)\n将构件分为三组，并严格执行几何参数映射：\n* **Group_A (Square/Rectangular)**: 方形/矩形。\n    - `b` = Width (宽度), `h` = Depth (深度).\n* **Group_B (Circular)**: 圆形。\n    - `b` = Diameter (直径 $D$), `h` = Diameter (直径 $D$). (Must satisfy `b == h`).\n* **Group_C (Round_ended)**: 圆端形/椭圆形。\n    - `b` = Major Axis (长轴), `h` = Minor Axis (短轴). (Must satisfy `b >= h`).\n\n# 2. Data Extraction Dictionary (Precise Definitions)\n请严格基于以下定义提取数据：\n\n* **Basic Info**:\n    - `ref_no`: Leave blank (Python will auto-fill filename).\\n    - `specimen_label`: The unique ID/Label of the specimen.\\n\n* **Material Properties**:\n    - `fc_value`: Concrete compressive strength **value** only (MPa).\\n    - `fc_type`: Description (e.g., \"Cube 150\", \"Cylinder 150x300\"，\"prism 150×150×300mm\").示例中均为标准的立方体,圆柱体或棱柱体(轴心)抗压强度。若文中未说明规格，则只描述\"cube\"，\"cylinder\"，\"prism\"。\\n    - `fy`: Yield strength of steel (MPa).\\n    - `r_ratio`: Recycled aggregate ratio (%). Fill `0` if normal concrete.\\n\n* **Geometric Dimensions**:\n    - `b` & `h`: See Section 1 rules (mm).\\n    - `t`: Thickness of the steel tube (mm).\\n    - `r0`: External corner/radius (mm). **Calculate strictly as follows**:\\n        - For **Group_A**: Always fill `0`.\\n        - For **Group_B**: Fill `h / 2` (i.e., Radius).\\n        - For **Group_C**: Fill `h / 2` (Radius of the circular ends).\\n    - `L`: Length of the specimen (mm).\\n\n* **Loading & Results**:\n    - `e1`, `e2`: Eccentricity (mm).e1为上端偏心，e2为下端偏心,如果未明确定义上下端偏心，则默认e1=e2=文中的偏心e(eccentricity). Axial = 0.\n    - `n_exp`: **Experimental** Ultimate Bearing Capacity ($N_{exp}$/Peak Load). Unit: kN. **Exclude** FEA/Calculated results.\\n\n* **Source Evidence (Visual Tracking)**:\n- `source_evidence`: **必须提供数据来源的视觉定位**。\\n- 格式： \"Page [X], Table [Y]\" 或 \"Page [X] text section\"。\\n- 目的：方便人工回溯检查。\\n* **Formatting**:\n    - `fcy150`: Always leave as empty string `\"\"`.\\n    - Remove units from numeric fields.\\n\n# 3. OCR Correction (Visual Assistant)\n\n由于你是通过视觉识别图片中的文字，请注意以下OCR纠错，但不要改变原始数据的含义：\\n\\n- 区分数字 `1` 和字母 `l/I`。\\n- 区分数字 `0` 和字母 `O`。\\n- 注意小数点 `.` 的位置（结合土木工程常识，例如 fc 不可能是 305 MPa）。\\n\n# 4. Output Format (JSON Only - STRICT RULES)\n**CRITICAL OUTPUT REQUIREMENTS:**\\n\\n1. **PURE JSON ONLY**: Output ONLY a raw JSON string. DO NOT wrap in Markdown code blocks (```json ... ```). DO NOT add any explanatory text before or after the JSON.\\n\\n2. **reason FIELD CONSTRAINT**: The `reason` field MUST be either:\\n   - An empty string: `\"\"` (preferred for valid data)\\n   - OR a brief explanation with MAXIMUM 10 words (e.g., \"Not experimental CFST column paper\")\\n   - DO NOT write long explanations or paragraphs in the reason field\\n\\n3. **JSON Structure**:\\n```json\\n{\\n  \"is_valid\": true/false,\\n  \"reason\": \"\",\\n  \"Group_A\": [ ... ],\\n  \"Group_B\": [ ... ],\\n  \"Group_C\": [ ... ]\\n}\\n```\\n\\n**EXAMPLES OF CORRECT OUTPUT:**\\n✅ `{\"is_valid\": true, \"reason\": \"\", \"Group_A\": [{\"ref_no\": \"\", \"specimen_label\": \"C-1\", ...}]}`\\n\\n❌ WRONG: ```json{...}``` (Markdown wrapping)\\n❌ WRONG: `{\"is_valid\": true, \"reason\": \"The paper provides comprehensive experimental data...\"}` (Too long)\\n\"\"\"\n```\n\n**Dependencies**：\n- `main.py`: 包含SYSTEM_PROMPT定义\n\n**Reuses**：\n- 现有的OpenAI客户端配置\n- 现有的JSON解析逻辑（增加截断检测）\n\n### Component 4: Configuration Schema Extension\n\n**Purpose**：扩展config.json结构，支持页面筛选配置\n\n**系统限制**：扫描的PDF页数至多30页（`absolute_max_pages: 30`），这是为了防止处理过大的PDF文件导致的性能问题。\n\n**数据模型**：\n```json\n{\n  \"api_settings\": {...},\n  \"processing_settings\": {\n    \"short_paper_threshold\": 10,  // ≤10页跳过筛选\n    \"max_scan_limit\": 10,  // 回退策略时的最大页数\n    \"enable_smart_filtering\": true,  // 新增：启用智能筛选\n    \"absolute_max_pages\": 30,  // 扫描的PDF页数至多30页\n    \"page_filtering\": {  // 新增评分配置\n      \"table_title_weight\": 10,\n      \"data_keyword_weight\": 5,\n      \"reference_weight\": -5,\n      \"table_patterns\": [\n        \"Table\\\\s+\\\\d+\",\n        \"Tab\\\\.\\\\s+\\\\d+\"\n      ],\n      \"data_keywords\": [\n        \"Specimen\",\n        \"Experimental\",\n        \"kN\",\n        \"mm\",\n        \"B/t\",\n        \"D/t\"\n      ],\n      \"reference_patterns\": [\n        \"References\",\n        \"Bibliography\",\n        \"REFERENCES\"\n      ],\n      \"max_selected_pages\": 8,  // 筛选后最多选择8页\n      \"mandatory_include_first_page\": true  // 强制包含第1页\n    },\n    \"absolute_max_pages\": 30  // 扫描的PDF页数至多30页\n  },\n  \"paths\": {...}\n}\n```\n\n## Data Models\n\n### Page Scoring Result\n```python\n@dataclass\nclass PageScore:\n    \"\"\"单页评分结果\"\"\"\n    page_number: int      # 页码（从1开始）\n    score: int            # 总得分\n    matched_keywords: List[str]  # 匹配的关键词列表\n    is_table_page: bool   # 是否包含表格\n    is_reference: bool    # 是否为参考文献页\n```\n\n### Smart Filtering Config\n```python\n@dataclass\nclass PageFilteringConfig:\n    \"\"\"页面筛选配置\"\"\"\n    enabled: bool = True  # 是否启用智能筛选\n    table_title_weight: int = 10\n    data_keyword_weight: int = 5\n    reference_weight: int = -5\n    max_selected_pages: int = 8  # 筛选后最多选择8页\n    mandatory_include_first_page: bool = True  # 强制包含第1页\n    table_patterns: List[str] = None\n    data_keywords: List[str] = None\n    reference_patterns: List[str] = None\n\n    def __init__(self, **kwargs):\n        # 设置默认值\n        self.table_patterns = [\n            r'Table\\\\s+\\\\d+',\n            r'Tab\\\\.\\\\s+\\\\d+',\n            r'表\\\\s+\\\\d+'\n        ]\n        self.data_keywords = [\n            'Specimen', 'Experimental', 'kN', 'mm',\n            'B/t', 'D/t', 'fc', 'fy', 'D', 't'\n        ]\n        self.reference_patterns = [\n            r'References', r'Bibliography', r'REFERENCES', r'参考文献'\n        ]\n\n        # 从kwargs更新，但保留未指定的默认值\n        for key, value in kwargs.items():\n            if key in ['table_patterns', 'data_keywords', 'reference_patterns']:\n                # 对于列表类型，合并而不是完全替换\n                if isinstance(value, list):\n                    existing = getattr(self, key)\n                    existing.extend(value)\n                    setattr(self, key, list(set(existing)))  # 去重\n            else:\n                setattr(self, key, value)\n```\n\n### Processing Debug Info\n```python\n@dataclass\nclass FilteringDebugInfo:\n    \"\"\"用于日志和调试的详细信息\"\"\"\n    total_pages: int\n    filtered_pages: List[int]\n    page_scores: List[PageScore]\n    strategy: str\n    processing_time_ms: float\n```\n\n## Error Handling\n\n### Error Scenario 1: pdfplumber提取失败\n\n**描述**：pdfplumber失败或无法提取文本，阶段一无法完成\n\n**处理方式**：\n```python\ntry:\n    page_texts = extract_page_texts(pdf_path)\n    if not page_texts:  # 提取失败为空\n        logger.warning(f\"文本提取失败，回退到简单截断策略: {pdf_path}\")\n        return list(range(1, min(page_count, max_scan_limit) + 1)), \"回退到截断策略\", []\nexcept Exception as e:\n    logger.error(f\"文本侦察阶段异常: {str(e)}\")\n    # 继续流程，但标记为回退状态\n```\n\n**用户影响**：\n- 系统降级处理（性能影响可忽略）\n- 日志中记录降级原因，无需用户干预\n\n### Error Scenario 2: JSON解析失败（截断）\n\n**描述**：AI返回的JSON因token溢出被截断，无法解析\n\n**处理策略**：\n```python\ntry:\n    response = client.chat.completions.create(...)\n    content = response.choices[0].message.content\n    # 检测是否可能是截断的JSON\n    if '\\\"Group_A\\\": [' in content and not content.rstrip().endswith('}'):\n        logger.error(f\"检测到截断JSON: {pdf_path}\")\n        # 移除包含Markdown代码块标记的错误内容\n    result = json.loads(content)\nexcept json.JSONDecodeError as e:\n    logger.error(f\"JSON解析失败: {str(e)}, 文件: {pdf_path}\")\n    # 移动到Manual_Review目录\n    move_to_manual_review(pdf_path, f\"JSON解析失败: {str(e)}\")\n    # 返回结果，继续批量处理\n    return {\"is_valid\": False, \"reason\": \"JSON Parsing Error\"}\n```\n\n**日志记录**：\n```\n[ERROR] JSON解析失败 - file: paper_2024.pdf\n错误详情: Unterminated string starting at: line 29 column 7\n截断内容: {\"is_valid\": true, \"reason\": \"The paper provides experimental data for eccentrically loaded...\n操作: 文件已移动到 Manual_Review/paper_2024.pdf\n```\n\n**用户影响**：\n- 打印红色错误消息\n- 文件被移动到Manual_Review目录\n- 批量处理继续，不影响其他文件\n\n### Error Scenario 3: 关键词配置错误\n\n**描述**：config.json中的关键词模式无效，导致匹配失败\n\n**处理方式**：\n```python\ntry:\n    pattern = re.compile(keyword_pattern)\nexcept re.error:\n    logger.warning(f\"无效的正则表达式模式: {keyword_pattern}, 跳过该模式\")\n    continue  # 跳过无效模式，其他模式继续匹配\n```\n\n**用户影响**：\n- 部分功能降级（某些关键词失效）\n- 日志中记录无效模式，便于调试\n- 核心功能不受影响\n\n### Error Scenario 4: 页面筛选为空\n\n**描述**：所有页面得分均为负数，筛选后无页面可选\n\n**处理策略**：\n```python\nfiltered_pages = [page for page, score in scored_pages if score > 0]\nif not filtered_pages:\n    logger.warning(f\"筛选后无有效页面，回退到默认页: {pdf_path}\")\n    # 选择得分最高的页面（即使为负），确保至少处理1页\n    filtered_pages = [scored_pages[0][0]]  # 加第一页\n```\n\n**用户影响**：\n- 处理效率降低（可能处理低价值页面）\n- 日志记录回退逻辑，提醒检查关键词配置\n\n## Testing Strategy\n\n### 单元测试\n\n**页面评分算法测试**：\n```python\ndef test_score_page_with_table():\n    text = \"Table 1 shows experimental results for Specimen C-1\"\n    score = score_page_content(text, DEFAULT_CONFIG)\n    assert score >= 15  # Table (+10) + Specimen (+5)\n\ndef test_score_reference_page():\n    text = \"References: [1] Smith et al. (2020)\"\n    score = score_page_content(text, DEFAULT_CONFIG)\n    assert score == -4  # Reference (-5) + Base (+1)\n\ndef test_score_empty_page():\n    text = \"\"\n    score = score_page_content(text, DEFAULT_CONFIG)\n    assert score == 0  # No content = No score\n```\n\n**页面选择逻辑测试**：\n```python\ndef test_select_top_pages():\n    scores = [(1, 20), (2, 5), (3, 15), (4, 25), (5, 10)]\n    selected = select_top_pages(scores, max_pages=3, include_first=True)\n    assert selected == [1, 4, 3]  # 第1页强制包含，然后选4和3\n\ndef test_empty_scores_fallback():\n    scores = []\n    selected = select_top_pages(scores, max_pages=5)\n    assert selected == []  # 空列表，触发回退逻辑\n```\n\n### 集成测试\n\n**两阶段处理完整流程**：\n```python\ndef test_two_phase_processing():\n    # 准备测试PDF（20页，包含表格在第5、12、18页）\n    pdf_path = \"test_long_paper.pdf\"\n    config = load_config_with_smart_filtering()\n\n    # 执行两阶段处理\n    pages, description, debug_info = get_smart_pages_to_process(pdf_path, config)\n\n    # 验证结果\n    assert len(pages) <= 8  # 不超过最大限制\n    assert 1 in pages  # 第1页强制包含\n    assert 5 in pages or 12 in pages or 18 in pages  # 至少选中一个含表格页\n```\n\n**JSON约束效果测试**：\n```python\ndef test_json_reason_field_constraint():\n    system_prompt = generate_system_prompt_with_constraints()\n\n    # 测试有效响应\n    valid_response = '{\"is_valid\": true, \"reason\": \"\", \"Group_A\": []}'\n    result = parse_ai_response(valid_response)\n    assert result.is_valid == True\n\n    # 测试无效响应（reason过长）\n    long_reason = 'A' * 100  # 100个字符\n    invalid_response = f'{{\"is_valid\": true, \"reason\": \"{long_reason}\"}}'\n    result = parse_ai_response(invalid_response)\n    assert result.reason_length <= 10  # 被截断或记录警告\n```\n\n### 端到端测试\n\n**典型长论文场景**：\n- 测试PDF：30页，试验数据在第5-8页（表格），第20页（结果）\n- 预期选择：第1、5、6、7、8、20页（约6页）\n- 成本节省：从10页减少到6页（40%节省）\n- 成功率：所有试验数据都应被提取\n\n**异常场景测试**：\n1. 使用失效的pdfplumber（模拟提取失败），验证回退策略\n2. 使用无效关键词配置（模拟正则错误），验证部分降级\n3. 使用被截断的AI响应，验证JSON解析错误处理\n\n**性能基准测试**：\n- 100页PDF的文本侦察时间应<200ms\n- 批量处理10个PDF（平均20页）的总时间影响<5%\n\n### 回归测试\n\n- 短论文（≤10页）必须保持全量扫描行为\n- 现有Excel数据格式必须保持不变\n- 零数据检测（空Group_A/B/C）逻辑不得被破坏\n",
  "fileStats": {
    "size": 20911,
    "lines": 555,
    "lastModified": "2026-01-06T16:16:58.024Z"
  },
  "comments": []
}