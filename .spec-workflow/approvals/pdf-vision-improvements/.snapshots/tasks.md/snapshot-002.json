{
  "id": "snapshot_1767716463294_zik35hgg6",
  "approvalId": "approval_1767716381646_qxkd6f3aa",
  "approvalTitle": "AI视觉处理改进实施任务 - 20个具体任务",
  "version": 2,
  "timestamp": "2026-01-06T16:21:03.294Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Tasks Document\n\n- [ ] 1.1 Install pdfplumber dependency and update requirements.txt\n  - Files: requirements.txt\n  - Install pdfplumber>=0.10.0 package\n  - Verify installation in CI/CD pipeline\n  - _Leverage: existing requirements.txt structure\n  - _Requirements: 2 (智能页面筛选机制)\n  - _Prompt: Role: Python DevOps Engineer specializing in dependency management | Task: Install pdfplumber package and update requirements.txt following requirement 2, ensuring proper version specification and compatibility with existing packages | Restrictions: Must not break existing dependencies, use version constraints for stability, verify installation in test environment | Success: pdfplumber is installed and importable, all existing tests pass, CI/CD pipeline completes successfully\n\n- [ ] 1.2 Create PDF text extraction utility module\n  - File: processing.py (add new functions at top)\n  - Implement extract_page_texts(pdf_path: str) -> Dict[int, str] function\n  - Use pdfplumber to extract text from all pages\n  - Add error handling for corrupted PDFs\n  - _Leverage: pdfplumber library, existing processing.py structure\n  - _Requirements: 2\n  - _Prompt: Role: Python Developer with expertise in PDF processing | Task: Create PDF text extraction utility using pdfplumber following requirement 2, implementing a function to extract text from all PDF pages with proper error handling | Restrictions: Must handle corrupted PDFs gracefully, return empty dict on failure, maintain backward compatibility, ensure memory efficiency for large PDFs | Success: Function extracts text from all pages correctly, handles errors robustly, processes 100-page PDF in <200ms, memory usage remains stable\n\n- [ ] 1.3 Implement page content scoring algorithm\n  - File: processing.py (after extract_page_texts)\n  - Implement score_page_content(text: str, config: dict) -> int function\n  - Apply scoring rules: Table titles (+10), Data keywords (+5), References (-5), Normal text (+1)\n  - Use compiled regex patterns for performance\n  - _Leverage: re module, config.json settings\n  - _Requirements: 2\n  - _Prompt: Role: Python Developer specializing in text processing and pattern matching | Task: Implement page scoring algorithm following requirement 2, applying weighted scoring based on keyword matches with optimized regex compilation | Restrictions: Must pre-compile regex patterns, handle unicode characters, support configurable weights, achieve O(n) complexity where n is text length | Success: Scores are calculated accurately, performance is <1ms per page, all pattern types detected correctly, weights are configurable via config.json\n\n- [ ] 1.4 Create main smart page filtering function\n  - File: processing.py\n  - Implement get_smart_pages_to_process(pdf_path: str, config: dict) -> Tuple[List[int], str, List[Dict]]\n  - Integrate extract_page_texts() and score_page_content()\n  - Apply logic: short papers (≤10 pages) skip filtering, select top pages by score, include page 1 mandatory\n  - _Leverage: extract_page_texts(), score_page_content(), get_page_count() from existing code\n  - _Requirements: 2\n  - _Prompt: Role: Python Developer with expertise in algorithm implementation | Task: Create main smart filtering function orchestrating text extraction and scoring following requirement 2, implementing page selection logic with fallback strategies | Restrictions: Must handle edge cases (all negative scores, empty text extraction), respect absolute_max_pages limit of 30, maintain logging of decisions, support both filtered and unfiltered modes | Success: Function returns correct pages for all scenarios, logs are clear and informative, handles edge cases gracefully, short papers bypass filtering correctly\n\n- [ ] 2.1 Update SYSTEM_PROMPT with JSON output constraints\n  - File: main.py (modify SYSTEM_PROMPT constant)\n  - Add complete SYSTEM_PROMPT with Role, Relevance Check, Task, and Output Format sections\n  - Include CRITICAL OUTPUT REQUIREMENTS section with JSON_ONLY and reason_field constraints\n  - Add EXAMPLES showing correct and incorrect outputs\n  - _Leverage: existing SYSTEM_PROMPT content, full prompt text from design.md\n  - _Requirements: 1\n  - _Prompt: Role: Prompt Engineer specializing in AI instruction design | Task: Update SYSTEM_PROMPT in main.py following requirement 1 with complete structured prompt including JSON output constraints and examples | Restrictions: Must preserve all existing CFST extraction logic, add constraints without breaking existing prompts, ensure clear separation of sections, maintain backwards compatibility | Success: Prompt is comprehensive and well-structured, JSON constraints are clearly specified, examples illustrate correct/incorrect formats, all existing extraction rules preserved\n\n- [ ] 2.2 Implement JSON response parsing with truncation detection\n  - File: processing.py\n  - Modify call_vision_api() or create parse_ai_response() function\n  - Detect truncated JSON by checking for unclosed braces or cut-off strings\n  - Implement fallback: if detected as truncated, move to Manual_Review\n  - _Leverage: json module, existing API response handling\n  - _Requirements: 1\n  - _Prompt: Role: Python Developer with expertise in error handling and API response parsing | Task: Implement JSON parsing with truncation detection following requirement 1, detecting and handling incomplete AI responses gracefully | Restrictions: Must not crash on malformed JSON, provide clear error messages, move failed files to Manual_Review directory, continue batch processing on failure | Success: Truncated JSON is detected accurately, failed files moved correctly, batch processing continues, error logs are informative\n\n- [ ] 2.3 Add JSON parse error handling in main processing loop\n  - File: main.py (in PDF processing loop)\n  - Wrap process_pdf() call in try/except for JSONDecodeError\n  - On JSON parse failure: log ERROR with filename and truncated content, move to Manual_Review\n  - Ensure batch processing continues\n  - _Leverage: existing JSON parsing, move_failed_file pattern\n  - _Requirements: 1\n  - _Prompt: Role: Python Developer with expertise in exception handling and batch processing | Task: Add JSON parsing error handling to main processing loop in main.py following requirement 1, implementing graceful failure and continuation | Restrictions: Must catch specific JSONDecodeError, log at ERROR level with helpful details, preserve failed file content, ensure other PDFs continue processing | Success: JSON errors are caught and logged, failed files moved to Manual_Review, remaining PDFs process successfully, system is resilient to API failures\n\n- [ ] 3.1 Extend config.json schema with page filtering settings\n  - File: config.json\n  - Add processing_settings.enable_smart_filtering: true\n  - Add processing_settings.absolute_max_pages: 30\n  - Add processing_settings.page_filtering section with all weights, patterns, and options\n  - Add comments explaining each configuration option\n  - _Leverage: existing config.json structure from design.md\n  - _Requirements: 2\n  - _Prompt: Role: Configuration Management Specialist | Task: Extend config.json schema with comprehensive page filtering settings following requirement 2 and design.md specifications, including all weights, patterns, and system limits | Restrictions: Must maintain backward compatibility with existing configs, use clear section organization, include inline comments, validate numeric ranges and pattern formats | Success: config.json validates successfully, all settings are documented, default values are sensible, structure is extensible for future additions\n\n- [ ] 3.2 Add config validation for new page filtering settings\n  - File: config_manager.py\n  - Add validation rules for page_filtering configuration\n  - Validate: weight values are numeric, patterns are valid regex, max_selected_pages is positive\n  - Provide clear error messages for invalid configurations\n  - _Leverage: existing config validation patterns in config_manager.py\n  - _Requirements: 1, 2\n  - _Prompt: Role: Python Developer specializing in configuration validation | Task: Add configuration validation rules for new page filtering settings following requirements 1 and 2, implementing comprehensive validation with helpful error messages | Restrictions: Must integrate with existing validation functions, provide Chinese error messages where appropriate, validate all configuration options, maintain validation performance | Success: Validation catches all configuration errors, error messages guide users to fix issues, config loading fails fast with clear diagnostics, validation is efficient\n\n- [ ] 4.1 Update pdf2image conversion calls to support page selection\n  - File: processing.py\n  - Modify convert_pdf_to_images() to accept selected_pages parameter\n  - Update get_pages_to_process() to use get_smart_pages_to_process()\n  - Ensure configuration drives behavior (enable_smart_filtering flag)\n  - _Leverage: existing convert_pdf_to_images(), get_pages_to_process(), process_pdf() functions\n  - _Requirements: 2\n  - _Prompt: Role: Python Developer with expertise in function modification and integration | Task: Update pdf2image conversion logic to support selective page processing following requirement 2, modifying existing functions to accept page lists and configuration-driven behavior | Restrictions: Must maintain backward compatibility when filtering is disabled, ensure function signatures remain clean, preserve existing DPI and format options, test with both filtered and unfiltered modes | Success: Functions accept page selection parameters, configuration controls filtering behavior, both modes work correctly, existing tests pass\n\n- [ ] 4.2 Modify process_pdf() to integrate smart filtering\n  - File: processing.py\n  - Update process_pdf() to call get_smart_pages_to_process() before image conversion\n  - Pass selected pages to convert_pdf_to_images()\n  - Log page selection decisions and scores to console\n  - _Leverage: process_pdf() existing structure, get_smart_pages_to_process(), logging utilities\n  - _Requirements: 2\n  - _Prompt: Role: Python Developer with expertise in workflow orchestration | Task: Modify process_pdf() main function to integrate smart page filtering following requirement 2, orchestrating text extraction, scoring, and selective image conversion | Restrictions: Must preserve existing API structure, maintain same return format, ensure logging is clear and informative, handle both short and long papers correctly | Success: process_pdf() uses smart filtering when enabled, logs page selections, maintains return format compatibility, all processing paths tested\n\n- [ ] 5.1 Write unit tests for page scoring algorithm\n  - File: tests/test_page_scoring.py (new)\n  - Test score_page_content() with various inputs: table pages, data pages, reference pages, mixed content\n  - Verify scoring accuracy and performance\n  - Test with both English and Chinese text\n  - _Leverage: pytest framework, score_page_content() function\n  - _Requirements: 2\n  - _Prompt: Role: QA Engineer with expertise in unit testing | Task: Write comprehensive unit tests for page scoring algorithm following requirement 2, covering all scoring scenarios and edge cases | Restrictions: Must test in isolation without PDF dependencies, cover all pattern types, verify performance benchmarks (<1ms per page), include unicode text tests | Success: All scoring scenarios pass tests, edge cases handled correctly, performance meets requirements, tests are maintainable and well-documented\n\n- [ ] 5.2 Write unit tests for smart page selection logic\n  - File: tests/test_smart_filtering.py (new)\n  - Test get_smart_pages_to_process() with mock data\n  - Verify short papers bypass filtering\n  - Verify top pages selected by score\n  - Test edge cases: all negative scores, empty extraction, tied scores\n  - _Leverage: pytest, unittest.mock, get_smart_pages_to_process()\n  - _Requirements: 2\n  - _Prompt: Role: QA Engineer specializing in algorithm testing | Task: Write unit tests for smart page selection logic following requirement 2, testing all decision paths and edge cases | Restrictions: Must mock pdfplumber and external dependencies, test both filtered and unfiltered modes, verify page 1 always included, test edge cases thoroughly | Success: All selection logic paths covered, edge cases handled correctly, tests demonstrate correct behavior for all scenarios, test suite runs quickly\n\n- [ ] 5.3 Write unit tests for JSON parsing error handling\n  - File: tests/test_json_parsing.py (new)\n  - Test truncated JSON detection\n  - Test successful parsing of valid JSON\n  - Test malformed JSON error handling\n  - Verify Manual_Review file movement on failure\n  - _Leverage: pytest, mock responses, existing test fixtures\n  - _Requirements: 1\n  - _Prompt: Role: QA Engineer with expertise in error handling testing | Task: Write unit tests for JSON parsing and error handling following requirement 1, covering success and failure scenarios | Restrictions: Must mock API responses, test truncation detection accuracy, verify logging and file operations, test continuation after failure | Success: Truncated JSON detected correctly, valid JSON parsed successfully, error handling tested comprehensively, file movements and logging verified\n\n- [ ] 6.1 Create integration test for two-phase processing workflow\n  - File: tests/test_integration.py (new or add to existing)\n  - Test complete workflow: PDF → text extraction → scoring → page selection → image conversion → AI processing\n  - Use real PDF samples (create test fixtures if needed)\n  - Verify cost savings and data accuracy\n  - _Leverage: pytest, test PDFs in fixtures/, process_pdf() function\n  - _Requirements: 1, 2\n  - _Prompt: Role: Integration Test Engineer | Task: Create comprehensive integration test for two-phase processing workflow following requirements 1 and 2, testing complete pipeline from PDF intake to AI response | Restrictions: Must use realistic test PDFs with known content, verify page selection accuracy, measure performance impact, ensure data extraction quality is maintained | Success: Integration test validates entire workflow, demonstrates cost savings, confirms data accuracy, performance benchmarks are met\n\n- [ ] 6.2 Create integration test for JSON constraint effectiveness\n  - File: tests/test_integration.py\n  - Test AI responses adhere to JSON constraints\n  - Verify reason field is empty or ≤10 words in practice\n  - Ensure no markdown wrapping in responses\n  - Track token usage reduction\n  - _Leverage: pytest, test_process_pdf.py patterns, real API calls\n  - _Requirements: 1\n  - _Prompt: Role: Integration Test Engineer focusing on API behavior | Task: Create integration test validating JSON constraint effectiveness following requirement 1, measuring actual AI response compliance | Restrictions: Must test with real API calls, verify multiple response samples, measure token usage before/after, ensure constraints are effective | Success: AI responses comply with constraints, reason fields are properly limited, no markdown wrapping, token usage reduced measurably\n\n- [ ] 7.1 Update processing.py module docstring and comments\n  - File: processing.py\n  - Add comprehensive docstring explaining vision-based processing\n  - Add inline comments for complex scoring logic\n  - Document function parameters and return types\n  - _Leverage: existing code, design.md documentation\n  - _Requirements: 1, 2\n  - _Prompt: Role: Technical Writer with Python expertise | Task: Update processing.py documentation following requirements 1 and 2, adding comprehensive docstrings and inline comments explaining vision-based processing and scoring logic | Restrictions: Must follow existing documentation style, explain complex algorithms clearly, maintain documentation consistency, use Google or Sphinx docstring format | Success: Docstrings are complete and informative, inline comments explain non-obvious logic, parameters and return types are documented, documentation enhances code maintainability\n\n- [ ] 7.2 Update main.py inline documentation\n  - File: main.py\n  - Add comments explaining JSON constraint additions to SYSTEM_PROMPT\n  - Document error handling logic for JSON parsing failures\n  - Update function docstrings for modified behavior\n  - _Leverage: existing code structure, full SYSTEM_PROMPT from design.md\n  - _Requirements: 1\n  - _Prompt: Role: Technical Writer specializing in inline documentation | Task: Update main.py inline documentation following requirement 1, explaining JSON constraints and error handling additions | Restrictions: Must not clutter code with excessive comments, focus on non-obvious logic, maintain consistency with existing style, explain error handling patterns clearly | Success: Comments explain constraint rationale, error handling logic is documented, code is maintainable and self-documenting where appropriate\n\n- [ ] 7.3 Update README.md with new architecture description\n  - File: README.md\n  - Document two-phase processing approach (text scouting + vision extraction)\n  - Explain smart page filtering benefits\n  - Add configuration examples for page filtering\n  - Document JSON constraint improvements\n  - _Leverage: existing README.md structure, design.md descriptions\n  - _Requirements: 1, 2\n  - _Prompt: Role: Technical Documentation Specialist | Task: Update README.md following requirements 1 and 2, documenting new two-phase architecture and smart page filtering with examples | Restrictions: Must maintain existing README organization, write for user audience (not developers), include clear examples, update any outdated architecture descriptions | Success: README accurately reflects new architecture, configuration examples are helpful, benefits of improvements are explained, users can understand and use new features\n\n- [ ] 8.1 Run full test suite and verify all tests pass\n  - Execute: pytest tests/\n  - Verify all unit tests pass\n  - Verify integration tests pass\n  - Check test coverage >80%\n  - _Leverage: pytest, coverage.py, existing test suite\n  - _Requirements: all\n  - _Prompt: Role: QA Lead with expertise in test execution and coverage analysis | Task: Run complete test suite and verify all tests pass following all requirements, ensuring comprehensive coverage | Restrictions: Must achieve >80% code coverage, all tests must pass, no flaky tests, performance benchmarks must be met, ensure tests run in isolation | Success: All tests pass reliably, coverage exceeds 80%, performance requirements are verified, test suite is maintainable and documented\n\n- [ ] 8.2 Perform manual testing with real PDF samples\n  - Test with minimum 5 real CFST papers\n  - Include mix of short (≤10 pages) and long (>10 pages) papers\n  - Verify data extraction accuracy is maintained or improved\n  - Confirm cost savings (fewer pages processed for long papers)\n  - Validate JSON constraints are effective\n  - Document any edge cases or issues found\n  - _Leverage: files/ directory with real PDFs, main.py process_pdf()\n  - _Requirements: all\n  - _Prompt: Role: QA Engineer with domain expertise in CFST research papers | Task: Perform manual testing with real CFST PDF samples following all requirements, validating accuracy and cost improvements | Restrictions: Must use real research papers, test diverse paper structures and lengths, verify extraction quality manually, document all findings, ensure system is production-ready | Success: All test papers process successfully, data extraction accuracy is verified, cost savings are demonstrated, edge cases are identified and handled, system is ready for production use\n",
  "fileStats": {
    "size": 19625,
    "lines": 197,
    "lastModified": "2026-01-06T16:19:18.108Z"
  },
  "comments": []
}